{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35536f6-166c-4b89-8136-96417db5be30",
   "metadata": {
    "id": "b35536f6-166c-4b89-8136-96417db5be30"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
    "\n",
    "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
    "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance? \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "w_A3rZOrIeQD",
   "metadata": {
    "id": "w_A3rZOrIeQD"
   },
   "outputs": [],
   "source": [
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "norm_style = Style(bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "pprint2 = partial(console.print, style=norm_style)\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEgV11oZmJGg",
   "metadata": {
    "id": "zEgV11oZmJGg"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Pre-Release Evaluation\n",
    "\n",
    "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
    "\n",
    "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
    "\n",
    "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
    "\n",
    "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
    "\n",
    "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
    "\n",
    "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
    "\n",
    "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
    "\n",
    "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
    "\n",
    "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
    "\n",
    "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** LLM-as-a-Judge Formulation\n",
    "\n",
    "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
    "\n",
    "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
    "\n",
    "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
    "\n",
    "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
    "\n",
    "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
    "\n",
    "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
    "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
    "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
    "\n",
    "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fDDNaBA9N3XM",
   "metadata": {
    "id": "fDDNaBA9N3XM"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
    "\n",
    "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/). \n",
    "\n",
    "**To prepare for our RAG chain evaluation, we will need to:**\n",
    "\n",
    "- Pull in our document index (the one we saved in the previous notebook).\n",
    "- Recreate our RAG pipeline of choice.\n",
    "\n",
    "**We will specifically be implementing a judge formulation with the following steps:**\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "**The chain should be a simple but powerful process that tests for the following objective:**\n",
    "\n",
    "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bh8jaOqak0f",
   "metadata": {
    "id": "1bh8jaOqak0f"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1:** Pull In Your Document Retrieval Index\n",
    "\n",
    "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tlE7a2lseLOy",
   "metadata": {
    "id": "tlE7a2lseLOy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore_index/\n",
      "docstore_index/index.pkl\n",
      "docstore_index/index.faiss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m238\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "\n",
      "Summary: Large pre-trained language models have been shown to store factual knowledge\n",
      "in their parameters, and achieve state-of-the-art results when fine-tuned on\n",
      "downstream NLP tasks. However, their ability to access and precisely manipulate\n",
      "knowledge is still limited, and hence on knowledge-intensive tasks, their\n",
      "performance lags behind task-specific architectures. Additionally, providing\n",
      "provenance for their decisions and updating their world knowledge remain open\n",
      "research problems. Pre-trained models with a differentiable access mechanism to\n",
      "explicit non-parametric memory can overcome this issue, but have so far been\n",
      "only investigated for extractive downstream tasks. We explore a general-purpose\n",
      "fine-tuning recipe for retrieval-augmented generation (RAG) -- models which\n",
      "combine pre-trained parametric and non-parametric memory for language\n",
      "generation. We introduce RAG models where the parametric memory is a\n",
      "pre-trained seq2seq model and the non-parametric memory is a dense vector index\n",
      "of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\n",
      "formulations, one which conditions on the same retrieved passages across the\n",
      "whole generated sequence, the other can use different passages per token. We\n",
      "fine-tune and evaluate our models on a wide range of knowledge-intensive NLP\n",
      "tasks and set the state-of-the-art on three open domain QA tasks, outperforming\n",
      "parametric seq2seq models and task-specific retrieve-and-extract architectures.\n",
      "For language generation tasks, we find that RAG models generate more specific,\n",
      "diverse and factual language than a state-of-the-art parametric-only seq2seq\n",
      "baseline.\n",
      "\n",
      "Page Body: .S.\\nRAG-T It\\u2019s the only U.S. state named for a U.S. president\\nRAG-S It\\u2019s the state where you\\u2019ll \\ufb01nd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante\\u2019s \\\"Inferno\\\" is the \\ufb01rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"\\nFor 2-way classi\\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations\n"
     ]
    }
   ],
   "source": [
    "## Make sure you have docstore_index.tgz in your working directory\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
    "\n",
    "!tar xzvf docstore_index.tgz\n",
    "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = list(docstore.docstore._dict.values())\n",
    "\n",
    "def format_chunk(doc):\n",
    "    return (\n",
    "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
    "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
    "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
    "    )\n",
    "\n",
    "## This printout just confirms that your store has been retrieved\n",
    "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "pprint(f\"Sample Chunk:\")\n",
    "print(format_chunk(docs[len(docs)//2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dib0F-t2N4LJ",
   "metadata": {
    "id": "dib0F-t2N4LJ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
    "\n",
    "Now that we have our index, we can recreate the RAG agent from the previous notebook! \n",
    "\n",
    "**Key Modifications:**\n",
    "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "XBi6Y8b8aXd2",
   "metadata": {
    "id": "XBi6Y8b8aXd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to know something interesting? Did you know that the document retrieval process can be a fascinating phenomenon? Let me give you an example!\n",
      "\n",
      "Imagine you're searching for a document online, and the algorithm kicks in to find relevant results. It's like a treasure hunt, but instead of digging, the algorithm is digging through millions of documents to find the ones that match what you're looking for.\n",
      "\n",
      "But here's the cool part: did you know that the Federal Reserve buying bonds in the secondary market can actually increase the money supply? It's true! According to Assistant B's response, this action can have several effects on the economy, including influencing interest rates, leading to inflation, and even impacting employment. It's like a domino effect, but instead of dominoes, it's all about financial dynamics!\n",
      "\n",
      "So, there you have it! That's something interesting about document retrieval and the financial world. Hope you found it as fascinating as I do!"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "llm = instruct_llm | StrOutputParser()\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
    "    \" The following information may be useful for your response: \"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    ")\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
    "    if isinstance(inputs, dict):\n",
    "        inputs = [inputs]\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "#####################################################################\n",
    "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
    "\n",
    "## Chain 1 Specs: \"Hello World\" -> retrieval_chain \n",
    "##   -> {'input': <str>, 'context' : <str>}\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)  ## GIVEN\n",
    "context_getter = RunnableLambda(lambda x: itemgetter('input') | docstore.as_retriever() | long_reorder | docs2str)  ## TODO\n",
    "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
    "\n",
    "## Chain 2 Specs: retrieval_chain -> generator_chain \n",
    "##   -> {\"output\" : <str>, ...} -> output_puller\n",
    "generator_chain = RunnableLambda(lambda x: chat_prompt | llm  )  ## TODO\n",
    "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller)  ## GIVEN\n",
    "\n",
    "## END TODO\n",
    "#####################################################################\n",
    "\n",
    "rag_chain = retrieval_chain | generator_chain\n",
    "\n",
    "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
    "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
    "\n",
    "In this section, we can implement the first few part of our evaluation routine:\n",
    "\n",
    "- **Sample the RAG agent document pool to find two document chunks.**\n",
    "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ymzuX-DSNvL6",
   "metadata": {
    "id": "ymzuX-DSNvL6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can large language models (LLMs) store and generate factual knowledge, and if so, how effective are they </span>\n",
       "<span style=\"font-weight: bold\">in completing tasks such as answering open-ended questions and conversational dialogue?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m store and generate factual knowledge, and if so, how effective are they \u001b[0m\n",
       "\u001b[1min completing tasks such as answering open-ended questions and conversational dialogue?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The answer to this question can be found in both documents. Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> suggests that strong LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">can match human preferences well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement, and can serve as a scalable and explainable way to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approximate human preferences. Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> goes a step further by exploring the idea of retrieval-augmented </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generation (RAG) models, which combine pre-trained parametric and non-parametric memory for language generation. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The authors of Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> demonstrate that RAG models can store and generate factual knowledge, and can outperform </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parametric seq2seq models on various knowledge-intensive NLP tasks. For example, when generating the title of a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">book, the RAG model can complete the title without depending on specific documents, suggesting that its parametric </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge is sufficient to complete the task.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: The answer to this question can be found in both documents. Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m suggests that strong LLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcan match human preferences well, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement, and can serve as a scalable and explainable way to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproximate human preferences. Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m goes a step further by exploring the idea of retrieval-augmented \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgeneration \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine pre-trained parametric and non-parametric memory for language generation. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe authors of Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m demonstrate that RAG models can store and generate factual knowledge, and can outperform \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mparametric seq2seq models on various knowledge-intensive NLP tasks. For example, when generating the title of a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbook, the RAG model can complete the title without depending on specific documents, suggesting that its parametric \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge is sufficient to complete the task.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: Can a neural network architecture that relies solely on attention mechanisms, such as the Transformer, </span>\n",
       "<span style=\"font-weight: bold\">efficiently handle and reason over complex knowledge and relationships like mathematical operations, which are a </span>\n",
       "<span style=\"font-weight: bold\">fundamental aspect of human knowledge and reasoning?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: Can a neural network architecture that relies solely on attention mechanisms, such as the Transformer, \u001b[0m\n",
       "\u001b[1mefficiently handle and reason over complex knowledge and relationships like mathematical operations, which are a \u001b[0m\n",
       "\u001b[1mfundamental aspect of human knowledge and reasoning?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the Modular Reasoning, Knowledge and Language (MRKL) system, a neural network architecture </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dubbed </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Jurassic-1\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> has demonstrated the ability to extract arithmetic operations in a few-shot setting, albeit </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with imperfect performance. However, when combined with data augmentation based on generating examples from a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">structured example space, it can achieve near-perfect performance in handling mathematical operations, suggesting </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that a Transformer-like architecture may be capable of efficiently handling complex knowledge and relationships, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">but with potential limitations in its ability to reason about abstract concepts without additional support and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">training.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: According to the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, a neural network architecture \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdubbed \u001b[0m\u001b[32m\"Jurassic-1\"\u001b[0m\u001b[1;38;2;118;185;0m has demonstrated the ability to extract arithmetic operations in a few-shot setting, albeit \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith imperfect performance. However, when combined with data augmentation based on generating examples from a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstructured example space, it can achieve near-perfect performance in handling mathematical operations, suggesting \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat a Transformer-like architecture may be capable of efficiently handling complex knowledge and relationships, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbut with potential limitations in its ability to reason about abstract concepts without additional support and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtraining.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How can we improve the limitations of large language models by adopting a systems approach that combines </span>\n",
       "<span style=\"font-weight: bold\">deep bidirectional representations, external knowledge sources, and discrete reasoning?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: How can we improve the limitations of large language models by adopting a systems approach that combines \u001b[0m\n",
       "\u001b[1mdeep bidirectional representations, external knowledge sources, and discrete reasoning?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: By defining a flexible architecture with multiple neural models, complemented by discrete knowledge and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">reasoning modules, such as the Modular Reasoning, Knowledge and Language (MRKL) system, which aims to address the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">limitations of large language models by incorporating knowledge and reasoning in addition to linguistic processing.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAnswer: By defining a flexible architecture with multiple neural models, complemented by discrete knowledge and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreasoning modules, such as the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, which aims to address the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlimitations of large language models by incorporating knowledge and reasoning in addition to linguistic processing.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_questions = 3\n",
    "synth_questions = []\n",
    "synth_answers = []\n",
    "\n",
    "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
    "\n",
    "for i in range(num_questions):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
    "    )\n",
    "    usr_msg = (\n",
    "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
    "        f\"Document2: {format_chunk(doc2)}\"\n",
    "    )\n",
    "\n",
    "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
    "    synth_questions += [qa_pair.split('\\n\\n')[0]]\n",
    "    synth_answers += [qa_pair.split('\\n\\n')[1]]\n",
    "    pprint2(f\"QA Pair {i+1}\")\n",
    "    pprint2(synth_questions[-1])\n",
    "    pprint(synth_answers[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5Q-3X4vS98P",
   "metadata": {
    "id": "c5Q-3X4vS98P"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 4:** Answer The Synthetic Questions\n",
    "\n",
    "In this section, we can implement the third part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- **Use the RAG agent to generate its own answer.**\n",
    "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7T3GSwhZPHjF",
   "metadata": {
    "id": "7T3GSwhZPHjF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"font-weight: bold\">Question: Can large language models (LLMs) store and generate factual knowledge, and if so, how effective are they </span>\n",
       "<span style=\"font-weight: bold\">in completing tasks such as answering open-ended questions and conversational dialogue?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\u001b[1mQuestion: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m store and generate factual knowledge, and if so, how effective are they \u001b[0m\n",
       "\u001b[1min completing tasks such as answering open-ended questions and conversational dialogue?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: You're interested in the capabilities of large language models (LLMs) when it comes to storing and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generating factual knowledge, as well as handling tasks like answering open-ended questions and engaging in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conversational dialogue. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Research suggests that LLMs can indeed store and generate factual knowledge, leveraging this for downstream NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. A notable study, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> found that pre-trained </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs can achieve state-of-the-art results after fine-tuning on specific tasks. However, current benchmarks for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assessing these capabilities might not be entirely adequate. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In terms of how effective LLMs are at completing tasks like answering open-ended questions and conversational </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dialogue, studies such as those mentioned in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> indicate that</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">current benchmarks often rely on narrow, specific questions that don't fully capture the complexity of real-world </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conversations. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Interestingly, another study presented in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">explored the effectiveness of retrieval-augmented models on tasks that require accessing and manipulating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge, achieving notable results in fact verification and retrieval tasks, including matching state-of-the-art </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on the FEVER fact verification dataset.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: You're interested in the capabilities of large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m when it comes to storing and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerating factual knowledge, as well as handling tasks like answering open-ended questions and engaging in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconversational dialogue. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mResearch suggests that LLMs can indeed store and generate factual knowledge, leveraging this for downstream NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks. A notable study, \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"\u001b[0m\u001b[1;38;2;118;185;0m found that pre-trained \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs can achieve state-of-the-art results after fine-tuning on specific tasks. However, current benchmarks for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0massessing these capabilities might not be entirely adequate. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn terms of how effective LLMs are at completing tasks like answering open-ended questions and conversational \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdialogue, studies such as those mentioned in \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m indicate that\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcurrent benchmarks often rely on narrow, specific questions that don't fully capture the complexity of real-world \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconversations. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInterestingly, another study presented in \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexplored the effectiveness of retrieval-augmented models on tasks that require accessing and manipulating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge, achieving notable results in fact verification and retrieval tasks, including matching state-of-the-art \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance on the FEVER fact verification dataset.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "<span style=\"font-weight: bold\">Question: Can a neural network architecture that relies solely on attention mechanisms, such as the Transformer, </span>\n",
       "<span style=\"font-weight: bold\">efficiently handle and reason over complex knowledge and relationships like mathematical operations, which are a </span>\n",
       "<span style=\"font-weight: bold\">fundamental aspect of human knowledge and reasoning?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\u001b[1mQuestion: Can a neural network architecture that relies solely on attention mechanisms, such as the Transformer, \u001b[0m\n",
       "\u001b[1mefficiently handle and reason over complex knowledge and relationships like mathematical operations, which are a \u001b[0m\n",
       "\u001b[1mfundamental aspect of human knowledge and reasoning?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the authors mention that the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture, which relies solely on attention mechanisms, has successfully handled and reasoned over complex tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like machine translation and English constituency parsing. However, regarding the ability to reason over </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mathematical operations, it's worth noting that there isn't a direct example provided in the paper.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But, let's dig a bit deeper. The authors do mention that the Transformer generalizes well to other tasks, including</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ones that may require handling mathematical operations. For instance, the paper mentions that end-to-end memory </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">networks, which are based on a recurrent attention mechanism, have been shown to perform well on simple-language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering and language modeling tasks. However, it's unclear whether these specific tasks involve </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mathematical operations.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Considering this, it's not a straightforward affirmative. The Transformer has demonstrated its ability to handle </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complex tasks, but whether it can efficiently reason over mathematical operations, which are a fundamental aspect </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of human knowledge and reasoning, remains unknown.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the paper \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m, the authors mention that the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture, which relies solely on attention mechanisms, has successfully handled and reasoned over complex tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike machine translation and English constituency parsing. However, regarding the ability to reason over \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmathematical operations, it's worth noting that there isn't a direct example provided in the paper.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBut, let's dig a bit deeper. The authors do mention that the Transformer generalizes well to other tasks, including\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mones that may require handling mathematical operations. For instance, the paper mentions that end-to-end memory \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnetworks, which are based on a recurrent attention mechanism, have been shown to perform well on simple-language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion answering and language modeling tasks. However, it's unclear whether these specific tasks involve \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmathematical operations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mConsidering this, it's not a straightforward affirmative. The Transformer has demonstrated its ability to handle \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplex tasks, but whether it can efficiently reason over mathematical operations, which are a fundamental aspect \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof human knowledge and reasoning, remains unknown.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "<span style=\"font-weight: bold\">Question: How can we improve the limitations of large language models by adopting a systems approach that combines </span>\n",
       "<span style=\"font-weight: bold\">deep bidirectional representations, external knowledge sources, and discrete reasoning?</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\u001b[1mQuestion: How can we improve the limitations of large language models by adopting a systems approach that combines \u001b[0m\n",
       "\u001b[1mdeep bidirectional representations, external knowledge sources, and discrete reasoning?\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MRKL Systems: A modular, neuro-symbolic architecture that combines large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models, external knowledge sources and discrete reasoning\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, large language models (LMs) have ushered in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new era for AI, but they are also inherently limited in several ways. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The key limitation mentioned is that it is not practical to fine-tune and serve multiple large models, and adding a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new task to a multi-task-trained LM can lead to catastrophic forgetting, requiring retraining on the entire task </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">set. This can be infeasible due to the cost of training such models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To overcome these limitations, the document proposes a systems approach by introducing the Modular Reasoning, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Knowledge and Language (MRKL) system, which is a flexible architecture that combines large language models, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">external knowledge sources, and discrete reasoning modules.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The MRKL system is designed to tackle the challenges of linguistic processing, knowledge, and reasoning by using </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multiple neural models, complemented by discrete knowledge and reasoning modules. This approach allows for more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient use of resources and enables the incorporation of new tasks without the need for extensive retraining. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In short, the MRKL system proposes a systems approach that combines deep bidirectional representations, external </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge sources, and discrete reasoning to improve the limitations of large language models.</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the document \u001b[0m\u001b[32m\"MRKL Systems: A modular, neuro-symbolic architecture that combines large \u001b[0m\n",
       "\u001b[32mlanguage models, external knowledge sources and discrete reasoning\"\u001b[0m\u001b[1;38;2;118;185;0m, large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have ushered in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew era for AI, but they are also inherently limited in several ways. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe key limitation mentioned is that it is not practical to fine-tune and serve multiple large models, and adding a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew task to a multi-task-trained LM can lead to catastrophic forgetting, requiring retraining on the entire task \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mset. This can be infeasible due to the cost of training such models.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo overcome these limitations, the document proposes a systems approach by introducing the Modular Reasoning, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mKnowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, which is a flexible architecture that combines large language models, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexternal knowledge sources, and discrete reasoning modules.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe MRKL system is designed to tackle the challenges of linguistic processing, knowledge, and reasoning by using \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmultiple neural models, complemented by discrete knowledge and reasoning modules. This approach allows for more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficient use of resources and enables the incorporation of new tasks without the need for extensive retraining. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn short, the MRKL system proposes a systems approach that combines deep bidirectional representations, external \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge sources, and discrete reasoning to improve the limitations of large language models.\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Generate some synthetic answers to the questions above.\n",
    "##   Try to use the same syntax as the cell above\n",
    "rag_answers = []\n",
    "for i, q in enumerate(synth_questions):\n",
    "    ## TODO: Compute the RAG Answer\n",
    "    rag_answer = rag_chain.invoke(q)\n",
    "    rag_answers += [rag_answer]\n",
    "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
    "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ho5cnN_Xt_yr",
   "metadata": {
    "id": "Ho5cnN_Xt_yr"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Step 5:** Implement A Human Preference Metric\n",
    "\n",
    "In this section, we can implement the fourth part of our evaluation routine:\n",
    "\n",
    "- Sample the RAG agent document pool to find two document chunks.\n",
    "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
    "- Use the RAG agent to generate its own answer.\n",
    "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
    "\n",
    "The chain should be a simple but powerful process that tests for the following objective:\n",
    "\n",
    "> Does my RAG chain outperform a narrow chatbot with limited document access?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sf6f2oFLuPtu",
   "metadata": {
    "id": "sf6f2oFLuPtu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can large language models (LLMs) store and generate factual knowledge, and if so, how effective</span>\n",
       "<span style=\"font-weight: bold\">are they in completing tasks such as answering open-ended questions and conversational dialogue?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m store and generate factual knowledge, and if so, how effective\u001b[0m\n",
       "\u001b[1mare they in completing tasks such as answering open-ended questions and conversational dialogue?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The answer to this question can be found in both documents. Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> suggests that strong </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs like GPT-</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> can match human preferences well, achieving over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">% agreement, and can serve as a scalable and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">explainable way to approximate human preferences. Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> goes a step further by exploring the idea of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieval-augmented generation (RAG) models, which combine pre-trained parametric and non-parametric memory for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language generation. The authors of Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> demonstrate that RAG models can store and generate factual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge, and can outperform parametric seq2seq models on various knowledge-intensive NLP tasks. For example, when</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generating the title of a book, the RAG model can complete the title without depending on specific documents, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">suggesting that its parametric knowledge is sufficient to complete the task.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The answer to this question can be found in both documents. Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m suggests that strong \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs like GPT-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m can match human preferences well, achieving over \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;38;2;118;185;0m% agreement, and can serve as a scalable and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexplainable way to approximate human preferences. Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m goes a step further by exploring the idea of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mretrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models, which combine pre-trained parametric and non-parametric memory for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage generation. The authors of Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m demonstrate that RAG models can store and generate factual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge, and can outperform parametric seq2seq models on various knowledge-intensive NLP tasks. For example, when\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerating the title of a book, the RAG model can complete the title without depending on specific documents, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuggesting that its parametric knowledge is sufficient to complete the task.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: You're interested in the capabilities of large language models (LLMs) when it comes to storing and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">generating factual knowledge, as well as handling tasks like answering open-ended questions and engaging in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conversational dialogue. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Research suggests that LLMs can indeed store and generate factual knowledge, leveraging this for downstream NLP </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. A notable study, </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> found that pre-trained </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">LLMs can achieve state-of-the-art results after fine-tuning on specific tasks. However, current benchmarks for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">assessing these capabilities might not be entirely adequate. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In terms of how effective LLMs are at completing tasks like answering open-ended questions and conversational </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dialogue, studies such as those mentioned in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> indicate that</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">current benchmarks often rely on narrow, specific questions that don't fully capture the complexity of real-world </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">conversations. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Interestingly, another study presented in </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">explored the effectiveness of retrieval-augmented models on tasks that require accessing and manipulating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge, achieving notable results in fact verification and retrieval tasks, including matching state-of-the-art </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance on the FEVER fact verification dataset.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: You're interested in the capabilities of large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m when it comes to storing and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mgenerating factual knowledge, as well as handling tasks like answering open-ended questions and engaging in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconversational dialogue. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mResearch suggests that LLMs can indeed store and generate factual knowledge, leveraging this for downstream NLP \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtasks. A notable study, \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"\u001b[0m\u001b[1;38;2;118;185;0m found that pre-trained \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLLMs can achieve state-of-the-art results after fine-tuning on specific tasks. However, current benchmarks for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0massessing these capabilities might not be entirely adequate. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn terms of how effective LLMs are at completing tasks like answering open-ended questions and conversational \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdialogue, studies such as those mentioned in \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\u001b[0m\u001b[1;38;2;118;185;0m indicate that\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcurrent benchmarks often rely on narrow, specific questions that don't fully capture the complexity of real-world \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconversations. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mInterestingly, another study presented in \u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexplored the effectiveness of retrieval-augmented models on tasks that require accessing and manipulating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge, achieving notable results in fact verification and retrieval tasks, including matching state-of-the-art \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperformance on the FEVER fact verification dataset.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">] Justification: The second answer introduces new information, including specific studies and </span>\n",
       "<span style=\"font-weight: bold\">research papers (</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"</span><span style=\"font-weight: bold\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Judging LLM-as-a-Judge with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">MT-Bench and Chatbot Arena\"</span><span style=\"font-weight: bold\">), which improves the accuracy and currency of the answer. However, this new information</span>\n",
       "<span style=\"font-weight: bold\">also introduces inconsistencies with the original question, as it does not directly answer the question about how </span>\n",
       "<span style=\"font-weight: bold\">effective LLMs are in completing tasks such as answering open-ended questions and conversational dialogue, but </span>\n",
       "<span style=\"font-weight: bold\">rather presents secondary information on the effectiveness of retrieval-augmented models on fact verification and </span>\n",
       "<span style=\"font-weight: bold\">retrieval tasks.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: The second answer introduces new information, including specific studies and \u001b[0m\n",
       "\u001b[1mresearch papers \u001b[0m\u001b[1m(\u001b[0m\u001b[32m\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\"\u001b[0m\u001b[1m \u001b[0m\u001b[32m\"Judging LLM-as-a-Judge with \u001b[0m\n",
       "\u001b[32mMT-Bench and Chatbot Arena\"\u001b[0m\u001b[1m)\u001b[0m\u001b[1m, which improves the accuracy and currency of the answer. However, this new information\u001b[0m\n",
       "\u001b[1malso introduces inconsistencies with the original question, as it does not directly answer the question about how \u001b[0m\n",
       "\u001b[1meffective LLMs are in completing tasks such as answering open-ended questions and conversational dialogue, but \u001b[0m\n",
       "\u001b[1mrather presents secondary information on the effectiveness of retrieval-augmented models on fact verification and \u001b[0m\n",
       "\u001b[1mretrieval tasks.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: Can a neural network architecture that relies solely on attention mechanisms, such as the </span>\n",
       "<span style=\"font-weight: bold\">Transformer, efficiently handle and reason over complex knowledge and relationships like mathematical operations, </span>\n",
       "<span style=\"font-weight: bold\">which are a fundamental aspect of human knowledge and reasoning?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: Can a neural network architecture that relies solely on attention mechanisms, such as the \u001b[0m\n",
       "\u001b[1mTransformer, efficiently handle and reason over complex knowledge and relationships like mathematical operations, \u001b[0m\n",
       "\u001b[1mwhich are a fundamental aspect of human knowledge and reasoning?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the Modular Reasoning, Knowledge and Language (MRKL) system, a neural network </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture dubbed </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Jurassic-1\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> has demonstrated the ability to extract arithmetic operations in a few-shot </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">setting, albeit with imperfect performance. However, when combined with data augmentation based on generating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">examples from a structured example space, it can achieve near-perfect performance in handling mathematical </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">operations, suggesting that a Transformer-like architecture may be capable of efficiently handling complex </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge and relationships, but with potential limitations in its ability to reason about abstract concepts </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">without additional support and training.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, a neural network \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture dubbed \u001b[0m\u001b[32m\"Jurassic-1\"\u001b[0m\u001b[1;38;2;118;185;0m has demonstrated the ability to extract arithmetic operations in a few-shot \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msetting, albeit with imperfect performance. However, when combined with data augmentation based on generating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexamples from a structured example space, it can achieve near-perfect performance in handling mathematical \u001b[0m\n",
       "\u001b[1;38;2;118;185;0moperations, suggesting that a Transformer-like architecture may be capable of efficiently handling complex \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge and relationships, but with potential limitations in its ability to reason about abstract concepts \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwithout additional support and training.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the paper </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Attention Is All You Need\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, the authors mention that the Transformer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architecture, which relies solely on attention mechanisms, has successfully handled and reasoned over complex tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">like machine translation and English constituency parsing. However, regarding the ability to reason over </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mathematical operations, it's worth noting that there isn't a direct example provided in the paper.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">But, let's dig a bit deeper. The authors do mention that the Transformer generalizes well to other tasks, including</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ones that may require handling mathematical operations. For instance, the paper mentions that end-to-end memory </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">networks, which are based on a recurrent attention mechanism, have been shown to perform well on simple-language </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">question answering and language modeling tasks. However, it's unclear whether these specific tasks involve </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mathematical operations.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Considering this, it's not a straightforward affirmative. The Transformer has demonstrated its ability to handle </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complex tasks, but whether it can efficiently reason over mathematical operations, which are a fundamental aspect </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of human knowledge and reasoning, remains unknown.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the paper \u001b[0m\u001b[32m\"Attention Is All You Need\"\u001b[0m\u001b[1;38;2;118;185;0m, the authors mention that the Transformer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitecture, which relies solely on attention mechanisms, has successfully handled and reasoned over complex tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlike machine translation and English constituency parsing. However, regarding the ability to reason over \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmathematical operations, it's worth noting that there isn't a direct example provided in the paper.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mBut, let's dig a bit deeper. The authors do mention that the Transformer generalizes well to other tasks, including\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mones that may require handling mathematical operations. For instance, the paper mentions that end-to-end memory \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnetworks, which are based on a recurrent attention mechanism, have been shown to perform well on simple-language \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquestion answering and language modeling tasks. However, it's unclear whether these specific tasks involve \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmathematical operations.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mConsidering this, it's not a straightforward affirmative. The Transformer has demonstrated its ability to handle \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcomplex tasks, but whether it can efficiently reason over mathematical operations, which are a fundamental aspect \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof human knowledge and reasoning, remains unknown.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">] The second answer introduces inconsistencies and is not better than the first answer. </span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Justification: Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> provides a specific example from the MRKL system, showcasing a neural network architecture </span>\n",
       "<span style=\"font-weight: bold\">that can handle mathematical operations. Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> mentions the Transformer's ability to handle complex tasks but </span>\n",
       "<span style=\"font-weight: bold\">includes speculative language and unclear examples. Furthermore, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> states that it's unclear whether the </span>\n",
       "<span style=\"font-weight: bold\">mentioned tasks involve mathematical operations, introducing uncertainty and inconsistencies.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer introduces inconsistencies and is not better than the first answer. \u001b[0m\n",
       "\n",
       "\u001b[1mJustification: Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m provides a specific example from the MRKL system, showcasing a neural network architecture \u001b[0m\n",
       "\u001b[1mthat can handle mathematical operations. Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m mentions the Transformer's ability to handle complex tasks but \u001b[0m\n",
       "\u001b[1mincludes speculative language and unclear examples. Furthermore, Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m states that it's unclear whether the \u001b[0m\n",
       "\u001b[1mmentioned tasks involve mathematical operations, introducing uncertainty and inconsistencies.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "\n",
       "<span style=\"font-weight: bold\">Question: Question: How can we improve the limitations of large language models by adopting a systems approach that</span>\n",
       "<span style=\"font-weight: bold\">combines deep bidirectional representations, external knowledge sources, and discrete reasoning?</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
       "\n",
       "\u001b[1mQuestion: Question: How can we improve the limitations of large language models by adopting a systems approach that\u001b[0m\n",
       "\u001b[1mcombines deep bidirectional representations, external knowledge sources, and discrete reasoning?\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: By defining a flexible architecture with multiple neural models, complemented by discrete </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge and reasoning modules, such as the Modular Reasoning, Knowledge and Language (MRKL) system, which aims to</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">address the limitations of large language models by incorporating knowledge and reasoning in addition to linguistic</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">processing.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSynth Answer: Answer: By defining a flexible architecture with multiple neural models, complemented by discrete \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge and reasoning modules, such as the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, which aims to\u001b[0m\n",
       "\u001b[1;38;2;118;185;0maddress the limitations of large language models by incorporating knowledge and reasoning in addition to linguistic\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mprocessing.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: According to the document </span><span style=\"color: #008000; text-decoration-color: #008000\">\"MRKL Systems: A modular, neuro-symbolic architecture that combines large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models, external knowledge sources and discrete reasoning\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, large language models (LMs) have ushered in a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new era for AI, but they are also inherently limited in several ways. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The key limitation mentioned is that it is not practical to fine-tune and serve multiple large models, and adding a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">new task to a multi-task-trained LM can lead to catastrophic forgetting, requiring retraining on the entire task </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">set. This can be infeasible due to the cost of training such models.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To overcome these limitations, the document proposes a systems approach by introducing the Modular Reasoning, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Knowledge and Language (MRKL) system, which is a flexible architecture that combines large language models, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">external knowledge sources, and discrete reasoning modules.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The MRKL system is designed to tackle the challenges of linguistic processing, knowledge, and reasoning by using </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">multiple neural models, complemented by discrete knowledge and reasoning modules. This approach allows for more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient use of resources and enables the incorporation of new tasks without the need for extensive retraining. </span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In short, the MRKL system proposes a systems approach that combines deep bidirectional representations, external </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge sources, and discrete reasoning to improve the limitations of large language models.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRAG Answer: According to the document \u001b[0m\u001b[32m\"MRKL Systems: A modular, neuro-symbolic architecture that combines large \u001b[0m\n",
       "\u001b[32mlanguage models, external knowledge sources and discrete reasoning\"\u001b[0m\u001b[1;38;2;118;185;0m, large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m have ushered in a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew era for AI, but they are also inherently limited in several ways. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe key limitation mentioned is that it is not practical to fine-tune and serve multiple large models, and adding a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnew task to a multi-task-trained LM can lead to catastrophic forgetting, requiring retraining on the entire task \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mset. This can be infeasible due to the cost of training such models.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo overcome these limitations, the document proposes a systems approach by introducing the Modular Reasoning, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mKnowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, which is a flexible architecture that combines large language models, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mexternal knowledge sources, and discrete reasoning modules.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mThe MRKL system is designed to tackle the challenges of linguistic processing, knowledge, and reasoning by using \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmultiple neural models, complemented by discrete knowledge and reasoning modules. This approach allows for more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mefficient use of resources and enables the incorporation of new tasks without the need for extensive retraining. \u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mIn short, the MRKL system proposes a systems approach that combines deep bidirectional representations, external \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mknowledge sources, and discrete reasoning to improve the limitations of large language models.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">] The second answer introduces inconsistencies with the ground truth. While it mentions the </span>\n",
       "<span style=\"font-weight: bold\">Modular Reasoning, Knowledge and Language (MRKL) system and its ability to combine deep bidirectional </span>\n",
       "<span style=\"font-weight: bold\">representations, external knowledge sources, and discrete reasoning, it is actually a quote from a separate </span>\n",
       "<span style=\"font-weight: bold\">document and not answering the question directly. Additionally, it introduces new information not present in the </span>\n",
       "<span style=\"font-weight: bold\">first answer, such as the cost of training large models and the challenges of linguistic processing, knowledge, and</span>\n",
       "<span style=\"font-weight: bold\">reasoning, which make the second answer inferior to the first.</span>\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m The second answer introduces inconsistencies with the ground truth. While it mentions the \u001b[0m\n",
       "\u001b[1mModular Reasoning, Knowledge and Language \u001b[0m\u001b[1m(\u001b[0m\u001b[1mMRKL\u001b[0m\u001b[1m)\u001b[0m\u001b[1m system and its ability to combine deep bidirectional \u001b[0m\n",
       "\u001b[1mrepresentations, external knowledge sources, and discrete reasoning, it is actually a quote from a separate \u001b[0m\n",
       "\u001b[1mdocument and not answering the question directly. Additionally, it introduces new information not present in the \u001b[0m\n",
       "\u001b[1mfirst answer, such as the cost of training large models and the challenges of linguistic processing, knowledge, and\u001b[0m\n",
       "\u001b[1mreasoning, which make the second answer inferior to the first.\u001b[0m\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Adapt this prompt for whichever LLM you're actually interested in using. \n",
    "## If it's llama, maybe system message would be good?\n",
    "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION: \n",
    "Evaluate the following Question-Answer pair for human preference and consistency.\n",
    "Assume the first answer is a ground truth answer and has to be correct.\n",
    "Assume the second answer may or may not be true.\n",
    "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
    "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
    "\n",
    "Output Format:\n",
    "[Score] Justification. Value must be contained in array.\n",
    "\n",
    "{qa_trio}\n",
    "\n",
    "EVALUATION: \n",
    "\"\"\")\n",
    "\n",
    "pref_score = []\n",
    "\n",
    "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
    "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
    "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
    "\n",
    "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
    "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
    "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
    "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
    "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3L_q6fMH3i6_",
   "metadata": {
    "id": "3L_q6fMH3i6_"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pref_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpref_score\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(pref_score)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreference Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpref_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "pref_score = sum((\"2\" in score) for score in pref_score) / len(pref_score)\n",
    "print(f\"Preference Score: {pref_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
   "metadata": {
    "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Advanced Formulations\n",
    "\n",
    "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action. \n",
    "\n",
    "With that being said, this metric was merely a product of us specifying:\n",
    "- **What kind of behavior is important for our pipeline to have?**\n",
    "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
    "\n",
    "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
    "\n",
    "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
    "\n",
    "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
    "\n",
    "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine. \n",
    "\n",
    "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
    "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
    "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Assessment]** Evaluating For Credit\n",
    "\n",
    "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
    "\n",
    "- **Make sure you're in the course environment**\n",
    "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
    "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
    "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
    "\n",
    "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference. \n",
    "\n",
    "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e300ed-951c-4006-ac54-cbbd41251707",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':8090';\n",
    "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
   "metadata": {
    "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
    "\n",
    "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
    "\n",
    "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
    "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
    "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
    "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
    "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
    "\n",
    "**Additionally, some key topics you may be interested in delving more into include:**\n",
    "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
    "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
    "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
    "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
    "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
   "metadata": {
    "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
